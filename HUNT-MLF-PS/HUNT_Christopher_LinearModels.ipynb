{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaline et Régression Logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be interested in the implementation of the perceptron algorithm (Rosenblatt, 68), Adaline (Widrow et Hoff, 60) and Logisitc Regression (Cox, 66) whose pseudo-code are the following:\n",
    "\n",
    "Perceptron:\n",
    "`Input: Train, eta, MaxEp\n",
    "init: w\n",
    "epoch = 0\n",
    "err = 1\n",
    "m = len(Train)\n",
    "while epoque <= MaxEp and err! = 0\n",
    "    err = 0\n",
    "    for i in 1: m\n",
    "        h <- w * x\n",
    "        if (y * h <= 0)\n",
    "            w <- w + eta * y * x\n",
    "            err <- err + 1\n",
    "     epoch <- epoch + 1\n",
    "output: w`\n",
    "\n",
    "Adaline:\n",
    "`input: Train, eta, MaxEp\n",
    "init : w\n",
    "epoque=0\n",
    "err=1\n",
    "m = len(Train)\n",
    "while epoque<=MaxEp and err!=0\n",
    "    err=0\n",
    "    for i in 1:m\n",
    "        h <- w*x\n",
    "        if(y*h<=0)\n",
    "           err <- err+1\n",
    "        w <- w + eta*(y-dp)*x\n",
    "     epoque <- epoque+1\n",
    "output: w`\n",
    "\n",
    "Logistic Regression:\n",
    "`input: Train, eta, MaxEp\n",
    "init : w\n",
    "epoque=0\n",
    "err=1\n",
    "m = len(Train)\n",
    "while epoque<=MaxEp and err!=0\n",
    "    err=0\n",
    "    for i in 1:m\n",
    "        choisir un exemple (x,y) de Train de façon aléatoire\n",
    "        h <- w*x\n",
    "        if(y*h<=0)\n",
    "           err <- err+1\n",
    "        w <- w + eta*y*(1-sigm(y*dp))*x\n",
    "     epoque <- epoque+1\n",
    "output: w`\n",
    "\n",
    "1. Create a list of 4 elements corresponding to the logical AND example called `Train`:\n",
    "$Train=\\{((+1,+1),+1),((-1,+1),-1),((-1,-1),-1),((+1,-1),-1)\\}$\n",
    "\n",
    "Each element of the list is a list which last characteristic is the class of the example and the first characteristics their coordinates.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1], [-1, 1, -1], [-1, -1, -1], [1, -1, -1]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train=[[1,1,1],[-1,1,-1],[-1,-1,-1],[1,-1,-1]] # To be filledt\n",
    "\n",
    "Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Code the Perceptron, Adaline and LR (Logistic regression) programs\n",
    "\n",
    "Hint: You can write a function that calculates the dot product between an example $\\mathbf{x} = (x_1, \\ldots, x_d)$ and the weight vector $\\mathbf{w} = (w_0, w_1, \\ldots, w_d)$: \n",
    "$ h(\\mathbf{x},\\mathbf{w}) = w_0 + \\ sum_ {j = 1} ^ d w_j x_j $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x,w):\n",
    "    # The prediction of the model\n",
    "    Pred=0.0\n",
    "    \n",
    "    return Pred\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def Perceptron(Train, eta, MaxEp):\n",
    "    # Perceptron Regression Algorithm \n",
    "    \n",
    "    # Extract X and Y from training set\n",
    "    Train = np.array(Train)\n",
    "    X = np.array(Train[:,:-1])\n",
    "    Y = np.array(Train[:,-1])\n",
    "    \n",
    "    \n",
    "    d=len(X[0])\n",
    "    m=len(X)\n",
    "    epoch = 0\n",
    "    err   = 1\n",
    "    \n",
    "    \n",
    "    # create weights\n",
    "    W      = np.zeros(X.shape[1]+1)\n",
    "    #W     = np.random.rand(X.shape[1]+1)* (0.9-0.01) + 0.05 ## Random weights?\n",
    "\n",
    "    while  epoch <= MaxEp and err != 0:\n",
    "        err = 0\n",
    "        for i in range(m):\n",
    "            h = W[0] + np.dot(X[i],W[1:])             \n",
    "            if (Y[i] * h <= 0):\n",
    "                W[0]  = W[0]  + eta * Y[i]\n",
    "                W[1:] = W[1:] + eta * Y[i] * X[i]\n",
    "        epoch = epoch +1\n",
    "   \n",
    "    return W\n",
    "  \n",
    "def Adaline(Train, eta, MaxEp):    \n",
    "    # Adaline Regression Algorithm \n",
    "    \n",
    "    # Extract X and Y from training set\n",
    "    Train = np.array(Train)\n",
    "    X = np.array(Train[:,:-1])\n",
    "    Y = np.array(Train[:,-1])\n",
    "    \n",
    "    \n",
    "    d=len(X[0])\n",
    "    m=len(X)\n",
    "    epoch = 0\n",
    "    err   = 1\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # create weights\n",
    "    W      = np.zeros(X.shape[1]+1)\n",
    "    #W     = np.random.rand(X.shape[1]+1)* (0.9-0.01) + 0.05 ## Random weights?\n",
    "\n",
    "    while  epoch <= MaxEp and err != 0:\n",
    "        err = 0\n",
    "        for i in range(m):\n",
    "            h = W[0] + np.dot(X[i],W[1:])             \n",
    "            if (Y[i] * h <= 0):\n",
    "                err   = err + 1\n",
    "                \n",
    "            W[0]  = W[0]  + eta * (Y[i] - h)\n",
    "            W[1:] = W[1:] + eta * (Y[i] - h) * X[i]\n",
    "        \n",
    "        epoch = epoch +1\n",
    "    return W    \n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def LR(Train, eta, MaxEp):    \n",
    "    import random\n",
    "    # Logisitc Regression Algorithm \n",
    "    \n",
    "    # Extract X and Y from training set\n",
    "    Train = np.array(Train)\n",
    "    X = np.array(Train[:,:-1])\n",
    "    Y = np.array(Train[:,-1])\n",
    "    \n",
    "    \n",
    "    d=len(X[0])\n",
    "    m=len(X)\n",
    "    epoch = 0\n",
    "    err   = 1\n",
    "    \n",
    "    \n",
    "    # create weights\n",
    "    W      = np.zeros(X.shape[1]+1)\n",
    "    #W     = np.random.rand(X.shape[1]+1)* (0.9-0.01) + 0.05 ## Random weights?\n",
    "\n",
    "    while  epoch <= MaxEp and err != 0:\n",
    "        err = 0\n",
    "        for i in range(m):\n",
    "            rand = random.randrange(m)\n",
    "            h = W[0] + np.dot(X[rand],W[1:])             \n",
    "            if (Y[rand] * h <= 0):\n",
    "                err   = err + 1\n",
    "#        w <- w + eta*y*(1-sigm(y*dp))*x\n",
    "            W[0]  = W[0]  + eta * Y[rand] * (1-sigmoid(Y[rand]*h))\n",
    "            W[1:] = W[1:] + eta * Y[rand] * (1-sigmoid(Y[rand]*h)) * X[rand]\n",
    "        epoch = epoch +1\n",
    "    return W    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply the three learning models on the logical AND, and calculate the model error rate on this basis.\n",
    "\n",
    "Hint: You can write a function that takes a weight vector $\\mathbf{w}$ and an example $(\\mathbf{x},y)$ and calculates the error rate of the model with weight $\\mathbf{w}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rates\n",
      "Perceptron :1.0\n",
      "Adaline :1.0\n",
      "LR :1.0\n"
     ]
    }
   ],
   "source": [
    "def calc_Error_Rate(W,X,Y):\n",
    "    eRate = 0\n",
    "    n     = len(X)\n",
    "    H     = W[0] + np.dot(X[:],W[1:]) #numpy array operations should be faster than repeated operations in a loop(I think?)\n",
    "    \n",
    "    for i in range(n):\n",
    "        h      = 1 if H[i] >0 else -1\n",
    "        eRate += 1 if h!=Y[i] else  0\n",
    "        \n",
    "    eRate = (n-eRate)/n\n",
    "    return eRate\n",
    "\n",
    "# Extract X and Y from training set\n",
    "Train = np.array(Train)\n",
    "X = np.array(Train[:,:-1])\n",
    "Y = np.array(Train[:,-1])\n",
    "\n",
    "print(\"Error Rates\")\n",
    "print(\"Perceptron :\",end='') \n",
    "print(calc_Error_Rate(Perceptron(Train, 0.1, 10),X,Y))\n",
    "\n",
    "print(\"Adaline :\",end='') \n",
    "print(calc_Error_Rate(Adaline(Train, 0.1, 10),X,Y))\n",
    "\n",
    "print(\"LR :\",end='') \n",
    "print(calc_Error_Rate(LR(Train, 0.1, 10),X,Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We are now going to focus on the behavior of the three models on http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks), https://archive.ics.uci.edu/ml/datasets/spambase, https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29, https://archive.ics.uci.edu/ml/datasets/Ionosphere. These files are in the current respository with the names `sonar.txt`; `spam.txt`; `wdbc.txt` and `ionoshpere.txt`. We can use the following `ReadCollection` function in order to read the files in the form of the training set that is requested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def Normalize(x):\n",
    "    norm=0.0\n",
    "    for e in x:\n",
    "        norm+=e**2\n",
    "    for i in range(len(x)):\n",
    "        x[i]/=sqrt(norm)\n",
    "    return x\n",
    "\n",
    "# Read wdbc.txt file in the Python format of request training set \n",
    "def ReadCollection(filename):\n",
    "    tag_df=pd.read_table(filename,sep=',',header=None)\n",
    "    Dic={'b': -1, 'g': +1}\n",
    "    X=[]\n",
    "    for e in range(len(tag_df)):\n",
    "        x=list(tag_df.loc[e,:])\n",
    "        #x.pop(0)\n",
    "        cls=x.pop()\n",
    "        x=Normalize(x)\n",
    "        x.insert(len(x),Dic[cls])\n",
    "        X.append(x)\n",
    "\n",
    "    \n",
    "    random.shuffle(X)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Run the three models on these files with $\\eta=0.01$ et $\\eta=0.1$ and `MaxEp=500`.\n",
    " \n",
    " 3. Report in the table below the average of the error rates on the test by repeating each experiment 20 times. \n",
    " \n",
    " <br>\n",
    " <br>\n",
    " \n",
    " \n",
    " <center> $\\eta=0.01$, MaxE$=500$ </center>\n",
    "    \n",
    "    \n",
    "  | Collection | Perceptron | Adaline |    RL    |\n",
    "  |------------|------------|---------|----------|\n",
    "  |   WDBC     |0.3258741259|0.0797203|0.09440560|                 \n",
    "  | Ionosphere |0.1880681818|0.0982954|0.06647727|\n",
    "  |   Sonar    |0.4048076923|0.2490385|0.23749999|\n",
    "  |   Spam     |0.39130.3913|0.2554300|0.21802780|\n",
    " \n",
    " <br><br>\n",
    "  \n",
    "  <center> $\\eta=0.1$, MaxEp$=500$ </center>\n",
    "    \n",
    "    \n",
    "  | Collection | Perceptron | Adaline |    RL    |\n",
    "  |------------|------------|---------|----------|\n",
    "  |   WDBC     |0.1765734266|0.0958042|0.08216783|                 \n",
    "  | Ionosphere |0.1982954545|0.1096590|0.08352272|\n",
    "  |   Sonar    |0.3634615385|0.2903846|0.24519231|\n",
    "  |   Spam     |0.3505647263|0.2675934|0.16646394|\n",
    "  \n",
    "  Hint: you can use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-cfe84106d682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mWLA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdaline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0merrA\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mEmpiricalRisk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWLA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mWLR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0merrL\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mEmpiricalRisk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b989f23b8513>\u001b[0m in \u001b[0;36mLR\u001b[0;34m(Train, eta, MaxEp)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mrand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0merr\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def EmpiricalRisk(Test,W):\n",
    "    E = 0.0\n",
    "    m = len(Test)\n",
    "    \n",
    "    Test = np.array(Test)\n",
    "    X    = np.array(Test[:,:-1])\n",
    "    Y    = np.array(Test[:,-1])\n",
    "    H    = W[0] + np.dot(X[:],W[1:]) #numpy array operations should be faster than repeated operations in a loop(I think?)\n",
    "    \n",
    "    for i in range(m):\n",
    "        h  = 1 if H[i] >0 else -1\n",
    "        E += 1 if h!=Y[i] else  0\n",
    "        \n",
    "    E = E/m\n",
    "    \n",
    "    return E\n",
    "\n",
    "X=ReadCollection(\"ionosphere.txt\")\n",
    "\n",
    "errP=errA=errL=0.0\n",
    "for i in range(20):\n",
    "    x_train ,x_test = train_test_split(X,test_size=0.25)\n",
    "    WLP=Perceptron(x_train,0.1,500)\n",
    "    errP+=EmpiricalRisk(x_test,WLP)\n",
    "    WLA=Adaline(x_train,0.1,500)\n",
    "    errA+=EmpiricalRisk(x_test,WLA)\n",
    "    WLR=LR(x_train,0.1,500)\n",
    "    errL+=EmpiricalRisk(x_test,WLR)\n",
    "    \n",
    "print(\"Err perceptron=\",errP/float(20),\"Err Adaline=\",errA/float(20),\"Err RL=\",errL/float(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
